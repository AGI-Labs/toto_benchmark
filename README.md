# Train Offline, Test Online: A Real Robot Learning Benchmark
<!-- TODO: add teaser figures, some setup/task images, etc  -->

## Installation
<!-- TODO: conda only instructions from the docker instructions?  -->
You can either use a local conda environment or docker setup

### Setup conda environment
1. Run ```source setup_toto_env.sh```

### Setup docker environment
1. Follow the instructions in ```docker_instructions.md```

If you are contributing models to TOTO, we suggest setting up a docker...

### TOTO Visual Representation Models
### TOTO Datasets
<!-- TODO: need to update the dataset link after google drive clean up -->
TOTO consists of two tabletop manipulations tasks, scooping and pouring. The datasets of the two tasks can be downloaded [here](https://link-url-here.org).

<!-- TODO: update link to dataset README.md file. May consider create a dataset/ folder and add the readme into the repo -->
We release the following datasets: 
- `cloud-dataset-scooping.zip`
- `cloud-dataset-pouring.zip`
- `scooping_parsed_with_embeddings_moco_conv5_robocloud.pkl`
- `pouring_parsed_with_embeddings_moco_conv5_robocloud.pkl`

For more detailed dataset format information, see [here](https://link-url-here.org).

## Train a TOTO BC Agent

```
python train.py --config-name train_bc.yaml data.pickle_fn=assets/cloud-dataset-scooping/scooping_parsed_with_embeddings_moco_conv5_robocloud.pkl
```

<!-- TODO: instructions on training agents with other vision representations? need to parse the dataset, etc -->

## Submission Instructions
### Contributing a Visual Representation Model
<!-- TODO: mention somewhere the assumption that our BC pipeline assume your image embedding to be a 1D vector? -->
- Download the datasets [here](https://link-url-here.org).
    - Note: You may or may not train your visual representation model on our datasets. However, the datasets are still needed for training BC agents for submission. 
- Create an `assets/` folder for storing datasets and models.
    ```
    cd toto_starter
    mkdir assets
    ```
- Move and unzip the datasets in 'assets/', so it has the following structure:
    ```
    assets/
    - cloud-dataset-scooping/
        - data/
    - cloud-dataset-pouring/
        - data/
    ```
- Train the model in your preferred way. You don't have to train it on our datasets, but if you want to, feel free to use the provided `example_load.py` for loading images in our datasets.
<!-- TODO: add example_load.py to github, and update this with a link -->

- After your model is trained, update the file `vision/CollaboratorEncoder.py` for loading your model, as well as any transforms needed.

- Update your model's embedding size in `vision/__init__.py`.
- Launch `data_with_embeddings.py` to generate a dataset with image embeddings generated by your model. 

    ```
    # Example command for the scooping dataset: 
    python data_with_embeddings.py --data_folder assets/cloud-dataset-scooping/ --vision_model collaborator_encoder 
    ```
    After this, a new data file `assets/cloud-dataset-scooping/parsed_with_embeddings_collaborator_encoder` will be generated. 
- Train a BC agent with this new data file.
    ```
    python train.py --config-name train_bc.yaml data.pickle_fn=assets/cloud-dataset-scooping/parsed_with_embeddings_collaborator_encoder.pkl data.vision_model='collaborator_encoder'
    ```
    A new agent folder will be created in `outputs/<path_to>/<agent>/`.
- Once the above is done, run `python test_stub_env.py -f outputs/<path_to>/<agent>/` for a simple simulated test on the robot. If everything works as expected, we are ready to have the agent to be evaluated on the real robot!
- For submission, **TODO (what files to submit? if zipping the whole folder, do we want to add in a bullet point of deleting the datasets in assets/ before zipping?)**

### Contributing an Agent
- Download the datasets [here](https://link-url-here.org) and train your agents in your preferred way.
- Update the agent file: `agents/CollaboratorAgent.py`. This acts as a wrapper around your model to interface with the robot stack. Please refer to `agents/Agent.py` for more information
- Update the agent config file `outputs/collaborator_agent/hydra.yaml` for initializing your agent
- Once the above is done, run `python test_stub_env.py -f outputs/collaborator_agent/` for a simple simulated test on the robot. If everything works as expected, we are ready to have the agent to be evaluated on the real robot!
- For submission, **TODO (zipping the whole folder should be fine? Maybe also add a note that we don't want them to submit the dataset, or any provided vision models)**

